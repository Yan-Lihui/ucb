{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“““Lab 3: Neural Networks”的副本”的副本”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yan-Lihui/ucb/blob/master/%E2%80%9C%E2%80%9C%E2%80%9CLab_3_Neural_Networks%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J05bjKF06Yxx",
        "colab_type": "text"
      },
      "source": [
        "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`, as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P792lM-N6WyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAME = \"Lihui Yan\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyb_RNpFreOr",
        "colab_type": "text"
      },
      "source": [
        "# Lab 3: Neural Networks #\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj9Uh79ereOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6SRFrhfreOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "0fe64951-1664-4e2c-f7d0-8dc62ede877a"
      },
      "source": [
        "!wget http://people.ischool.berkeley.edu/~zp/course_datasets/lab_4_training.csv\n",
        "!wget http://people.ischool.berkeley.edu/~zp/course_datasets/lab_4_test.csv\n",
        "df_train = pd.read_csv('./lab_4_training.csv', index_col=0)\n",
        "df_train.loc[df_train['year'] == 'first\"', 'year'] = 'first'\n",
        "df_test = pd.read_csv('./lab_4_test.csv', index_col=0)\n",
        "df_test.loc[df_test['year'] == 'first\"', 'year'] = 'first'\n",
        "df_train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-10 11:24:13--  http://people.ischool.berkeley.edu/~zp/course_datasets/lab_4_training.csv\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 105581 (103K) [text/csv]\n",
            "Saving to: ‘lab_4_training.csv’\n",
            "\n",
            "\rlab_4_training.csv    0%[                    ]       0  --.-KB/s               \rlab_4_training.csv   92%[=================>  ]  95.40K   454KB/s               \rlab_4_training.csv  100%[===================>] 103.11K   491KB/s    in 0.2s    \n",
            "\n",
            "2020-08-10 11:24:13 (491 KB/s) - ‘lab_4_training.csv’ saved [105581/105581]\n",
            "\n",
            "--2020-08-10 11:24:14--  http://people.ischool.berkeley.edu/~zp/course_datasets/lab_4_test.csv\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26523 (26K) [text/csv]\n",
            "Saving to: ‘lab_4_test.csv’\n",
            "\n",
            "lab_4_test.csv      100%[===================>]  25.90K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-08-10 11:24:14 (371 KB/s) - ‘lab_4_test.csv’ saved [26523/26523]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>year</th>\n",
              "      <th>eyecolor</th>\n",
              "      <th>height</th>\n",
              "      <th>miles</th>\n",
              "      <th>brothers</th>\n",
              "      <th>sisters</th>\n",
              "      <th>computertime</th>\n",
              "      <th>exercise</th>\n",
              "      <th>exercisehours</th>\n",
              "      <th>musiccds</th>\n",
              "      <th>playgames</th>\n",
              "      <th>watchtv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>male</td>\n",
              "      <td>20</td>\n",
              "      <td>second</td>\n",
              "      <td>green</td>\n",
              "      <td>73.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>5.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>male</td>\n",
              "      <td>20</td>\n",
              "      <td>third</td>\n",
              "      <td>other</td>\n",
              "      <td>71.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>male</td>\n",
              "      <td>22</td>\n",
              "      <td>fourth</td>\n",
              "      <td>hazel</td>\n",
              "      <td>75.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>male</td>\n",
              "      <td>19</td>\n",
              "      <td>second</td>\n",
              "      <td>brown</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>20.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>male</td>\n",
              "      <td>22</td>\n",
              "      <td>fourth</td>\n",
              "      <td>hazel</td>\n",
              "      <td>71.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     gender  age    year eyecolor  ...  exercisehours  musiccds  playgames  watchtv\n",
              "1303   male   20  second    green  ...            5.0      50.0        1.0     15.0\n",
              "36     male   20   third    other  ...            4.0      10.0        0.0      1.0\n",
              "489    male   22  fourth    hazel  ...            2.0     150.0        1.0     10.0\n",
              "1415   male   19  second    brown  ...            5.0     100.0        0.0      7.0\n",
              "616    male   22  fourth    hazel  ...            7.0      10.0        0.0      5.0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGt_10ZAreOv",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 1###\n",
        "Calculate a baseline accuracy measure using the majority class, assuming a target variable of 'gender'. The majority class is the most common value of the target variable in a particular dataset. Accuracy is calculated as (true positives + true negatives) / (all negatives and positives)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPiLgekreOw",
        "colab_type": "text"
      },
      "source": [
        "**Question 1.a**  \n",
        "Find the majority class in the training set. If you always predicted this class in the training set, what would your accuracy be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYjEFc1greOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb23d410-a509-437c-87e9-f2ef60ab2642"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "if len(df_train[df_train['gender']=='female'])>=len(df_train[df_train['gender']=='male']):\n",
        "  accuracy_num = len(df_train[df_train['gender']=='female'])/len(df_train['gender'])\n",
        "  print(accuracy_num)\n",
        "else:\n",
        "  accuracy_num = len(df_train[df_train['gender']=='male'])/len(df_train['gender'])\n",
        "  print(accuracy_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5377358490566038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULPKW0IvreOy",
        "colab_type": "text"
      },
      "source": [
        "**Question 1.b**   \n",
        "If you always predicted this same class (majority from the training set) in the test set, what would your accuracy be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dfU5mwh405vq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ee76848-5e67-4c69-9a4a-341ce5877572"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_class_te=df_test.groupby('gender').size()\n",
        "df_class_tr=df_train.groupby('gender').size()\n",
        "major_class=df_class_tr.idxmax()\n",
        "print(df_class_te[major_class]/df_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5226130653266332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKb2Ju-GreO0",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 2 ###\n",
        "Get started with Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYI6e3F3reO0",
        "colab_type": "text"
      },
      "source": [
        "   \n",
        "Choose a NN implementation (eg: scikit-learn) and specify which you choose. Be sure the implementation allows you to modify the number of hidden layers and hidden nodes per layer.  \n",
        "\n",
        "NOTE: When possible, specify the logsig (sigmoid/logistc) function as the transfer function (another word for activation function) and use Levenberg-Marquardt backpropagation (lbfgs). It is possible to specify logistic in Sklearn MLPclassifier (Neural net).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4am3sGc4reO1",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.a**   \n",
        "Train a neural network with a single 10 node hidden layer. Only use the Height feature of the dataset to predict the Gender. You will have to change Gender to a 0 and 1 class. After training, use your trained model to predict the class using the height feature from the training set. What was the accuracy of this prediction?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbAzltaw067l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f986ff8c-9eb9-4a64-cb2a-b1da1bc6ceb7"
      },
      "source": [
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=100,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "df_gender_test_=df_test['gender'].replace('male',1).replace('female',0)\n",
        "y_train=np.array(df_gender_train_)\n",
        "y_test=np.array(df_gender_test_)\n",
        "x_train=np.array(df_train['height']).reshape(-1,1)\n",
        "x_test=np.array(df_test['height']).reshape(-1,1)\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "print('training accuracy:',accuracy_score(y_train,y_pred_train))\n",
        "print('testing accuracy:',accuracy_score(y_test,y_pred_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.26984076\n",
            "Iteration 2, loss = 0.71556620\n",
            "Iteration 3, loss = 0.71537436\n",
            "Iteration 4, loss = 0.71477023\n",
            "Iteration 5, loss = 0.71395444\n",
            "Iteration 6, loss = 0.71315364\n",
            "Iteration 7, loss = 0.71229744\n",
            "Iteration 8, loss = 0.71145589\n",
            "Iteration 9, loss = 0.71066096\n",
            "Iteration 10, loss = 0.70990195\n",
            "Iteration 11, loss = 0.70913174\n",
            "Iteration 12, loss = 0.70843005\n",
            "Iteration 13, loss = 0.70770449\n",
            "Iteration 14, loss = 0.70706090\n",
            "Iteration 15, loss = 0.70641146\n",
            "Iteration 16, loss = 0.70577975\n",
            "Iteration 17, loss = 0.70519049\n",
            "Iteration 18, loss = 0.70461960\n",
            "Iteration 19, loss = 0.70406374\n",
            "Iteration 20, loss = 0.70353888\n",
            "Iteration 21, loss = 0.70301639\n",
            "Iteration 22, loss = 0.70253419\n",
            "Iteration 23, loss = 0.70204630\n",
            "Iteration 24, loss = 0.70160769\n",
            "Iteration 25, loss = 0.70114393\n",
            "Iteration 26, loss = 0.70073453\n",
            "Iteration 27, loss = 0.70032967\n",
            "Iteration 28, loss = 0.69995335\n",
            "Iteration 29, loss = 0.69956662\n",
            "Iteration 30, loss = 0.69921343\n",
            "Iteration 31, loss = 0.69886081\n",
            "Iteration 32, loss = 0.69850892\n",
            "Iteration 33, loss = 0.69822614\n",
            "Iteration 34, loss = 0.69788487\n",
            "Iteration 35, loss = 0.69758363\n",
            "Iteration 36, loss = 0.69732291\n",
            "Iteration 37, loss = 0.69704453\n",
            "Iteration 38, loss = 0.69677822\n",
            "Iteration 39, loss = 0.69650444\n",
            "Iteration 40, loss = 0.69627399\n",
            "Iteration 41, loss = 0.69605751\n",
            "Iteration 42, loss = 0.69581658\n",
            "Iteration 43, loss = 0.69561721\n",
            "Iteration 44, loss = 0.69537989\n",
            "Iteration 45, loss = 0.69518940\n",
            "Iteration 46, loss = 0.69500073\n",
            "Iteration 47, loss = 0.69482579\n",
            "Iteration 48, loss = 0.69462851\n",
            "Iteration 49, loss = 0.69447260\n",
            "Iteration 50, loss = 0.69430699\n",
            "Iteration 51, loss = 0.69414746\n",
            "Iteration 52, loss = 0.69398664\n",
            "Iteration 53, loss = 0.69385756\n",
            "Iteration 54, loss = 0.69371693\n",
            "Iteration 55, loss = 0.69357371\n",
            "Iteration 56, loss = 0.69343831\n",
            "Iteration 57, loss = 0.69331880\n",
            "Iteration 58, loss = 0.69320214\n",
            "Iteration 59, loss = 0.69308869\n",
            "Iteration 60, loss = 0.69297693\n",
            "Iteration 61, loss = 0.69287534\n",
            "Iteration 62, loss = 0.69277825\n",
            "Iteration 63, loss = 0.69267069\n",
            "Iteration 64, loss = 0.69257828\n",
            "Iteration 65, loss = 0.69249870\n",
            "Iteration 66, loss = 0.69240991\n",
            "Iteration 67, loss = 0.69231619\n",
            "Iteration 68, loss = 0.69224287\n",
            "Iteration 69, loss = 0.69216945\n",
            "Iteration 70, loss = 0.69209229\n",
            "Iteration 71, loss = 0.69201927\n",
            "Iteration 72, loss = 0.69195936\n",
            "Iteration 73, loss = 0.69188217\n",
            "Iteration 74, loss = 0.69183257\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "training accuracy: 0.5377358490566038\n",
            "testing accuracy: 0.5226130653266332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkqzIeshreO2",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.b**  \n",
        "Take the trained model from question 2.a and use it to predict the test set. This can be accomplished by taking the trained model and giving it the Height feature values from the test set. What is the accuracy of this model on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tw25ezWp07hj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0567ea5c-62d3-4d58-a06e-adbdcb1442e5"
      },
      "source": [
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=100,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "df_gender_test_=df_test['gender'].replace('male',1).replace('female',0)\n",
        "y_train=np.array(df_gender_train_)[:398]\n",
        "y_test=np.array(df_gender_test_)[:399]\n",
        "x_train=np.array(df_test['height'])[:398].reshape(-1,1)\n",
        "x_test=np.array(df_test['height'])[:399].reshape(-1,1)\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "print('testing accuracy:',accuracy_score(y_test,y_pred_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 7.48365279\n",
            "Iteration 2, loss = 3.64619383\n",
            "Iteration 3, loss = 0.87507866\n",
            "Iteration 4, loss = 0.72436660\n",
            "Iteration 5, loss = 0.72458038\n",
            "Iteration 6, loss = 0.72470738\n",
            "Iteration 7, loss = 0.72473897\n",
            "Iteration 8, loss = 0.72471010\n",
            "Iteration 9, loss = 0.72462325\n",
            "Iteration 10, loss = 0.72451865\n",
            "Iteration 11, loss = 0.72432948\n",
            "Iteration 12, loss = 0.72415857\n",
            "Iteration 13, loss = 0.72394614\n",
            "Iteration 14, loss = 0.72371536\n",
            "Iteration 15, loss = 0.72349125\n",
            "Iteration 16, loss = 0.72322550\n",
            "Iteration 17, loss = 0.72297782\n",
            "Iteration 18, loss = 0.72272845\n",
            "Iteration 19, loss = 0.72244332\n",
            "Iteration 20, loss = 0.72215198\n",
            "Iteration 21, loss = 0.72191021\n",
            "Iteration 22, loss = 0.72162548\n",
            "Iteration 23, loss = 0.72138348\n",
            "Iteration 24, loss = 0.72108325\n",
            "Iteration 25, loss = 0.72082322\n",
            "Iteration 26, loss = 0.72050885\n",
            "Iteration 27, loss = 0.72025626\n",
            "Iteration 28, loss = 0.71999118\n",
            "Iteration 29, loss = 0.71972704\n",
            "Iteration 30, loss = 0.71947409\n",
            "Iteration 31, loss = 0.71920350\n",
            "Iteration 32, loss = 0.71893410\n",
            "Iteration 33, loss = 0.71867543\n",
            "Iteration 34, loss = 0.71840779\n",
            "Iteration 35, loss = 0.71816619\n",
            "Iteration 36, loss = 0.71793583\n",
            "Iteration 37, loss = 0.71765649\n",
            "Iteration 38, loss = 0.71740275\n",
            "Iteration 39, loss = 0.71716081\n",
            "Iteration 40, loss = 0.71693661\n",
            "Iteration 41, loss = 0.71668765\n",
            "Iteration 42, loss = 0.71646227\n",
            "Iteration 43, loss = 0.71621905\n",
            "Iteration 44, loss = 0.71596053\n",
            "Iteration 45, loss = 0.71574342\n",
            "Iteration 46, loss = 0.71549615\n",
            "Iteration 47, loss = 0.71528318\n",
            "Iteration 48, loss = 0.71503134\n",
            "Iteration 49, loss = 0.71482929\n",
            "Iteration 50, loss = 0.71459462\n",
            "Iteration 51, loss = 0.71437490\n",
            "Iteration 52, loss = 0.71416484\n",
            "Iteration 53, loss = 0.71396225\n",
            "Iteration 54, loss = 0.71377144\n",
            "Iteration 55, loss = 0.71352479\n",
            "Iteration 56, loss = 0.71331752\n",
            "Iteration 57, loss = 0.71309202\n",
            "Iteration 58, loss = 0.71289687\n",
            "Iteration 59, loss = 0.71270437\n",
            "Iteration 60, loss = 0.71248753\n",
            "Iteration 61, loss = 0.71230988\n",
            "Iteration 62, loss = 0.71210283\n",
            "Iteration 63, loss = 0.71188878\n",
            "Iteration 64, loss = 0.71171224\n",
            "Iteration 65, loss = 0.71151696\n",
            "Iteration 66, loss = 0.71133669\n",
            "Iteration 67, loss = 0.71113936\n",
            "Iteration 68, loss = 0.71094783\n",
            "Iteration 69, loss = 0.71077790\n",
            "Iteration 70, loss = 0.71058770\n",
            "Iteration 71, loss = 0.71037775\n",
            "Iteration 72, loss = 0.71021550\n",
            "Iteration 73, loss = 0.71004379\n",
            "Iteration 74, loss = 0.70987509\n",
            "Iteration 75, loss = 0.70969429\n",
            "Iteration 76, loss = 0.70952048\n",
            "Iteration 77, loss = 0.70934078\n",
            "Iteration 78, loss = 0.70917768\n",
            "Iteration 79, loss = 0.70900928\n",
            "Iteration 80, loss = 0.70882692\n",
            "Iteration 81, loss = 0.70868250\n",
            "Iteration 82, loss = 0.70852423\n",
            "Iteration 83, loss = 0.70834691\n",
            "Iteration 84, loss = 0.70819709\n",
            "Iteration 85, loss = 0.70804369\n",
            "Iteration 86, loss = 0.70787664\n",
            "Iteration 87, loss = 0.70772498\n",
            "Iteration 88, loss = 0.70755729\n",
            "Iteration 89, loss = 0.70740650\n",
            "Iteration 90, loss = 0.70725681\n",
            "Iteration 91, loss = 0.70709283\n",
            "Iteration 92, loss = 0.70697935\n",
            "Iteration 93, loss = 0.70680739\n",
            "Iteration 94, loss = 0.70666854\n",
            "Iteration 95, loss = 0.70653921\n",
            "Iteration 96, loss = 0.70638390\n",
            "Iteration 97, loss = 0.70623984\n",
            "Iteration 98, loss = 0.70609649\n",
            "Iteration 99, loss = 0.70596549\n",
            "Iteration 100, loss = 0.70581981\n",
            "testing accuracy: 0.5226130653266332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMmIfsNEreO3",
        "colab_type": "text"
      },
      "source": [
        "**Question 2.c**   \n",
        "Neural Networks tend to prefer smaller, normalized feature values. Try taking the log of the height feature in both training and testing sets or use StandardScaler and MinMaxScaler in SKlearn to centre and normalize the data between 0-1 for continuous values. Repeat question 2.a and 2.b with the log version or the normalized and centered version of this feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wDhCZPaU07_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2437cebb-6e0e-4c85-d679-87750b5e50ac"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=100,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "df_gender_test_=df_test['gender'].replace('male',1).replace('female',0)\n",
        "y_train=np.array(df_gender_train_)\n",
        "y_test=np.array(df_gender_test_)\n",
        "x_train=np.array(df_train['height']).reshape(-1,1)\n",
        "x_test=np.array(df_test['height']).reshape(-1,1)\n",
        "scaler = StandardScaler()\n",
        "x_train_ss=scaler.fit_transform(x_train)\n",
        "x_test_ss=scaler.fit_transform(x_test)\n",
        "scaler_02=MinMaxScaler()\n",
        "x_train=scaler.fit_transform(x_train_ss)\n",
        "x_test=scaler.fit_transform(x_test_ss)\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "\n",
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=100,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "y_train_b=np.array(df_gender_train_)[:398]\n",
        "y_test_b=np.array(df_gender_test_)[:399]\n",
        "x_train_b=np.array(df_test['height'])[:398].reshape(-1,1)\n",
        "x_test_b=np.array(df_test['height'])[:399].reshape(-1,1)\n",
        "scaler = StandardScaler()\n",
        "x_train_ssb=scaler.fit_transform(x_train_b)\n",
        "x_test_ssb=scaler.fit_transform(x_test_b)\n",
        "scaler_02b=MinMaxScaler()\n",
        "x_train_b=scaler.fit_transform(x_train_ssb)\n",
        "x_test_b=scaler.fit_transform(x_test_ssb)\n",
        "clf2.fit(x_train_b,y_train_b)\n",
        "y_pred_train_b=clf2.predict(x_train_b)\n",
        "y_pred_test_b=clf2.predict(x_test_b)\n",
        "print('testing accuracy:',accuracy_score(y_test_b,y_pred_test_b))\n",
        "\n",
        "print('2.a training accuracy:',accuracy_score(y_train,y_pred_train))\n",
        "print('2.a testing accuracy:',accuracy_score(y_test,y_pred_test))\n",
        "print('2.b testing accuracy:',accuracy_score(y_test_b,y_pred_test_b))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.65537437\n",
            "Iteration 2, loss = 0.64954733\n",
            "Iteration 3, loss = 0.64194038\n",
            "Iteration 4, loss = 0.63363483\n",
            "Iteration 5, loss = 0.62545414\n",
            "Iteration 6, loss = 0.61736496\n",
            "Iteration 7, loss = 0.60962808\n",
            "Iteration 8, loss = 0.60226275\n",
            "Iteration 9, loss = 0.59511987\n",
            "Iteration 10, loss = 0.58823900\n",
            "Iteration 11, loss = 0.58152077\n",
            "Iteration 12, loss = 0.57506341\n",
            "Iteration 13, loss = 0.56889786\n",
            "Iteration 14, loss = 0.56280618\n",
            "Iteration 15, loss = 0.55695513\n",
            "Iteration 16, loss = 0.55121049\n",
            "Iteration 17, loss = 0.54559198\n",
            "Iteration 18, loss = 0.54012239\n",
            "Iteration 19, loss = 0.53495490\n",
            "Iteration 20, loss = 0.52983268\n",
            "Iteration 21, loss = 0.52494382\n",
            "Iteration 22, loss = 0.51998273\n",
            "Iteration 23, loss = 0.51540412\n",
            "Iteration 24, loss = 0.51080960\n",
            "Iteration 25, loss = 0.50637359\n",
            "Iteration 26, loss = 0.50206680\n",
            "Iteration 27, loss = 0.49789853\n",
            "Iteration 28, loss = 0.49391250\n",
            "Iteration 29, loss = 0.48990909\n",
            "Iteration 30, loss = 0.48612809\n",
            "Iteration 31, loss = 0.48239645\n",
            "Iteration 32, loss = 0.47875757\n",
            "Iteration 33, loss = 0.47533947\n",
            "Iteration 34, loss = 0.47194883\n",
            "Iteration 35, loss = 0.46870132\n",
            "Iteration 36, loss = 0.46547344\n",
            "Iteration 37, loss = 0.46248906\n",
            "Iteration 38, loss = 0.45946396\n",
            "Iteration 39, loss = 0.45656922\n",
            "Iteration 40, loss = 0.45379088\n",
            "Iteration 41, loss = 0.45117252\n",
            "Iteration 42, loss = 0.44846922\n",
            "Iteration 43, loss = 0.44598786\n",
            "Iteration 44, loss = 0.44349507\n",
            "Iteration 45, loss = 0.44117123\n",
            "Iteration 46, loss = 0.43892629\n",
            "Iteration 47, loss = 0.43679083\n",
            "Iteration 48, loss = 0.43465454\n",
            "Iteration 49, loss = 0.43261411\n",
            "Iteration 50, loss = 0.43067298\n",
            "Iteration 51, loss = 0.42881059\n",
            "Iteration 52, loss = 0.42694965\n",
            "Iteration 53, loss = 0.42517436\n",
            "Iteration 54, loss = 0.42347483\n",
            "Iteration 55, loss = 0.42186312\n",
            "Iteration 56, loss = 0.42023525\n",
            "Iteration 57, loss = 0.41868572\n",
            "Iteration 58, loss = 0.41720133\n",
            "Iteration 59, loss = 0.41579682\n",
            "Iteration 60, loss = 0.41439666\n",
            "Iteration 61, loss = 0.41303349\n",
            "Iteration 62, loss = 0.41181300\n",
            "Iteration 63, loss = 0.41051175\n",
            "Iteration 64, loss = 0.40926180\n",
            "Iteration 65, loss = 0.40813318\n",
            "Iteration 66, loss = 0.40699350\n",
            "Iteration 67, loss = 0.40591546\n",
            "Iteration 68, loss = 0.40487359\n",
            "Iteration 69, loss = 0.40384635\n",
            "Iteration 70, loss = 0.40285701\n",
            "Iteration 71, loss = 0.40189583\n",
            "Iteration 72, loss = 0.40099191\n",
            "Iteration 73, loss = 0.40010795\n",
            "Iteration 74, loss = 0.39922764\n",
            "Iteration 75, loss = 0.39841192\n",
            "Iteration 76, loss = 0.39765722\n",
            "Iteration 77, loss = 0.39683492\n",
            "Iteration 78, loss = 0.39608676\n",
            "Iteration 79, loss = 0.39532952\n",
            "Iteration 80, loss = 0.39463672\n",
            "Iteration 81, loss = 0.39397255\n",
            "Iteration 82, loss = 0.39330680\n",
            "Iteration 83, loss = 0.39268414\n",
            "Iteration 84, loss = 0.39206002\n",
            "Iteration 85, loss = 0.39146091\n",
            "Iteration 86, loss = 0.39089014\n",
            "Iteration 87, loss = 0.39033999\n",
            "Iteration 88, loss = 0.38976825\n",
            "Iteration 89, loss = 0.38927968\n",
            "Iteration 90, loss = 0.38879316\n",
            "Iteration 91, loss = 0.38825893\n",
            "Iteration 92, loss = 0.38780233\n",
            "Iteration 93, loss = 0.38735905\n",
            "Iteration 94, loss = 0.38690267\n",
            "Iteration 95, loss = 0.38648294\n",
            "Iteration 96, loss = 0.38605847\n",
            "Iteration 97, loss = 0.38566827\n",
            "Iteration 98, loss = 0.38526550\n",
            "Iteration 99, loss = 0.38492432\n",
            "Iteration 100, loss = 0.38453870\n",
            "Iteration 1, loss = 0.66037115\n",
            "Iteration 2, loss = 0.65966747\n",
            "Iteration 3, loss = 0.65858749\n",
            "Iteration 4, loss = 0.65726203\n",
            "Iteration 5, loss = 0.65563508\n",
            "Iteration 6, loss = 0.65391820\n",
            "Iteration 7, loss = 0.65201949\n",
            "Iteration 8, loss = 0.65004339\n",
            "Iteration 9, loss = 0.64799777\n",
            "Iteration 10, loss = 0.64586246\n",
            "Iteration 11, loss = 0.64379395\n",
            "Iteration 12, loss = 0.64156581\n",
            "Iteration 13, loss = 0.63938035\n",
            "Iteration 14, loss = 0.63716992\n",
            "Iteration 15, loss = 0.63497108\n",
            "Iteration 16, loss = 0.63286283\n",
            "Iteration 17, loss = 0.63057601\n",
            "Iteration 18, loss = 0.62841359\n",
            "Iteration 19, loss = 0.62622141\n",
            "Iteration 20, loss = 0.62406790\n",
            "Iteration 21, loss = 0.62190254\n",
            "Iteration 22, loss = 0.61981283\n",
            "Iteration 23, loss = 0.61765042\n",
            "Iteration 24, loss = 0.61558809\n",
            "Iteration 25, loss = 0.61351102\n",
            "Iteration 26, loss = 0.61147528\n",
            "Iteration 27, loss = 0.60947033\n",
            "Iteration 28, loss = 0.60743959\n",
            "Iteration 29, loss = 0.60546781\n",
            "Iteration 30, loss = 0.60353073\n",
            "Iteration 31, loss = 0.60159646\n",
            "Iteration 32, loss = 0.59963964\n",
            "Iteration 33, loss = 0.59770301\n",
            "Iteration 34, loss = 0.59582165\n",
            "Iteration 35, loss = 0.59392638\n",
            "Iteration 36, loss = 0.59205017\n",
            "Iteration 37, loss = 0.59019579\n",
            "Iteration 38, loss = 0.58840862\n",
            "Iteration 39, loss = 0.58655039\n",
            "Iteration 40, loss = 0.58470056\n",
            "Iteration 41, loss = 0.58292924\n",
            "Iteration 42, loss = 0.58112542\n",
            "Iteration 43, loss = 0.57937432\n",
            "Iteration 44, loss = 0.57761306\n",
            "Iteration 45, loss = 0.57589328\n",
            "Iteration 46, loss = 0.57413986\n",
            "Iteration 47, loss = 0.57238990\n",
            "Iteration 48, loss = 0.57073431\n",
            "Iteration 49, loss = 0.56900465\n",
            "Iteration 50, loss = 0.56733265\n",
            "Iteration 51, loss = 0.56565986\n",
            "Iteration 52, loss = 0.56398165\n",
            "Iteration 53, loss = 0.56237853\n",
            "Iteration 54, loss = 0.56071441\n",
            "Iteration 55, loss = 0.55913760\n",
            "Iteration 56, loss = 0.55747559\n",
            "Iteration 57, loss = 0.55589681\n",
            "Iteration 58, loss = 0.55431526\n",
            "Iteration 59, loss = 0.55273275\n",
            "Iteration 60, loss = 0.55117814\n",
            "Iteration 61, loss = 0.54960867\n",
            "Iteration 62, loss = 0.54808580\n",
            "Iteration 63, loss = 0.54656202\n",
            "Iteration 64, loss = 0.54505126\n",
            "Iteration 65, loss = 0.54353736\n",
            "Iteration 66, loss = 0.54206006\n",
            "Iteration 67, loss = 0.54056330\n",
            "Iteration 68, loss = 0.53905304\n",
            "Iteration 69, loss = 0.53758242\n",
            "Iteration 70, loss = 0.53614784\n",
            "Iteration 71, loss = 0.53466388\n",
            "Iteration 72, loss = 0.53323763\n",
            "Iteration 73, loss = 0.53182930\n",
            "Iteration 74, loss = 0.53042387\n",
            "Iteration 75, loss = 0.52899750\n",
            "Iteration 76, loss = 0.52761035\n",
            "Iteration 77, loss = 0.52619863\n",
            "Iteration 78, loss = 0.52484253\n",
            "Iteration 79, loss = 0.52346100\n",
            "Iteration 80, loss = 0.52209763\n",
            "Iteration 81, loss = 0.52074485\n",
            "Iteration 82, loss = 0.51939669\n",
            "Iteration 83, loss = 0.51808878\n",
            "Iteration 84, loss = 0.51676476\n",
            "Iteration 85, loss = 0.51541891\n",
            "Iteration 86, loss = 0.51415904\n",
            "Iteration 87, loss = 0.51282037\n",
            "Iteration 88, loss = 0.51155445\n",
            "Iteration 89, loss = 0.51027903\n",
            "Iteration 90, loss = 0.50900040\n",
            "Iteration 91, loss = 0.50771465\n",
            "Iteration 92, loss = 0.50649573\n",
            "Iteration 93, loss = 0.50521673\n",
            "Iteration 94, loss = 0.50399593\n",
            "Iteration 95, loss = 0.50279143\n",
            "Iteration 96, loss = 0.50153933\n",
            "Iteration 97, loss = 0.50032723\n",
            "Iteration 98, loss = 0.49911249\n",
            "Iteration 99, loss = 0.49792743\n",
            "Iteration 100, loss = 0.49673304\n",
            "testing accuracy: 0.8316582914572864\n",
            "2.a training accuracy: 0.8465408805031447\n",
            "2.a testing accuracy: 0.8542713567839196\n",
            "2.b testing accuracy: 0.8316582914572864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_SlOdcarePC",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 3###\n",
        "The rest of features in this dataset barring a few are categorical. No ML method accepts categorical features, so transform year, eyecolor, exercise into a set of binary features, one feature per unique original feature value, and mark the binary feature as ‘1’ if the feature value matches the original value and ‘0’ otherwise. Using only these binary variable transformed features, train and predict the class of the test set. What was your accuracy using Neural Network with a single 10 node hidden layer? During training, use a maximum number of iterations of 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YjhzBFNV1Aip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "73c35146-f497-48de-b533-7669122dc88b"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "le = LabelEncoder()\n",
        "df_category_train=df_train\n",
        "df_label_train=df_train[['year','eyecolor','exercise']]\n",
        "df_category_train1=df_category_train['year']\n",
        "df_category_train2=df_category_train['eyecolor']\n",
        "df_category_train3=df_category_train['exercise']\n",
        "df_label_train['year']=le.fit_transform(df_category_train1)\n",
        "df_label_train['eyecolor']=le.fit_transform(df_category_train2)\n",
        "df_label_train['exercise']=le.fit_transform(df_category_train3)\n",
        "df_onehot_train1=pd.get_dummies(df_label_train['year'])\n",
        "df_onehot_train2=pd.get_dummies(df_label_train['eyecolor'])\n",
        "df_onehot_train3=pd.get_dummies(df_label_train['exercise'])\n",
        "df_onehot_train_=pd.concat([df_label_train,df_onehot_train1,df_onehot_train2,df_onehot_train3],axis=1)\n",
        "df_onehot_train_.drop(['year','eyecolor','exercise'],axis=1,inplace=True)\n",
        "df_onehot_train=df_onehot_train_.loc[577:,:]\n",
        "df_onehot_test=df_onehot_train_.loc[:532,:]\n",
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=50,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "y_train=np.array(df_gender_train_.loc[577:])\n",
        "y_test=np.array(df_gender_train_.loc[:532])\n",
        "x_train=df_onehot_train.iloc[:,:].values\n",
        "x_test=df_onehot_test.iloc[:,:].values\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "print('training accuracy:',accuracy_score(y_train,y_pred_train))\n",
        "print('testing accuracy:',accuracy_score(y_test,y_pred_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.71713584\n",
            "Iteration 2, loss = 0.71652701\n",
            "Iteration 3, loss = 0.71569823\n",
            "Iteration 4, loss = 0.71480777\n",
            "Iteration 5, loss = 0.71377101\n",
            "Iteration 6, loss = 0.71290654\n",
            "Iteration 7, loss = 0.71191914\n",
            "Iteration 8, loss = 0.71115312\n",
            "Iteration 9, loss = 0.71033610\n",
            "Iteration 10, loss = 0.70961934\n",
            "Iteration 11, loss = 0.70890250\n",
            "Iteration 12, loss = 0.70820580\n",
            "Iteration 13, loss = 0.70764415\n",
            "Iteration 14, loss = 0.70706885\n",
            "Iteration 15, loss = 0.70648568\n",
            "Iteration 16, loss = 0.70597283\n",
            "Iteration 17, loss = 0.70548082\n",
            "Iteration 18, loss = 0.70496849\n",
            "Iteration 19, loss = 0.70459541\n",
            "Iteration 20, loss = 0.70420147\n",
            "Iteration 21, loss = 0.70376802\n",
            "Iteration 22, loss = 0.70339011\n",
            "Iteration 23, loss = 0.70304532\n",
            "Iteration 24, loss = 0.70269421\n",
            "Iteration 25, loss = 0.70237939\n",
            "Iteration 26, loss = 0.70206548\n",
            "Iteration 27, loss = 0.70176702\n",
            "Iteration 28, loss = 0.70148819\n",
            "Iteration 29, loss = 0.70126037\n",
            "Iteration 30, loss = 0.70100154\n",
            "Iteration 31, loss = 0.70071318\n",
            "Iteration 32, loss = 0.70048614\n",
            "Iteration 33, loss = 0.70027936\n",
            "Iteration 34, loss = 0.70005743\n",
            "Iteration 35, loss = 0.69982935\n",
            "Iteration 36, loss = 0.69960616\n",
            "Iteration 37, loss = 0.69942218\n",
            "Iteration 38, loss = 0.69922532\n",
            "Iteration 39, loss = 0.69906644\n",
            "Iteration 40, loss = 0.69886530\n",
            "Iteration 41, loss = 0.69869658\n",
            "Iteration 42, loss = 0.69852722\n",
            "Iteration 43, loss = 0.69835737\n",
            "Iteration 44, loss = 0.69821882\n",
            "Iteration 45, loss = 0.69804318\n",
            "Iteration 46, loss = 0.69790246\n",
            "Iteration 47, loss = 0.69778721\n",
            "Iteration 48, loss = 0.69763408\n",
            "Iteration 49, loss = 0.69748652\n",
            "Iteration 50, loss = 0.69736319\n",
            "training accuracy: 0.5209731543624161\n",
            "testing accuracy: 0.5376884422110553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSSr9sBlrePG",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 4###\n",
        "Using a NN, report the accuracy on  the test set of a model that trained only on the height and the eye color features of instances in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMNSlOmJrePG",
        "colab_type": "text"
      },
      "source": [
        "**Question 4.a**  \n",
        "What is the accuracy on the test set using the original height values (no pre-processing) and eye color as a one-hot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F_vN4tyv1Ckq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "8787961a-428d-4b47-8e82-74d9186628cb"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "X = df_train[['height', 'eyecolor']]\n",
        "X['eyecolor'] = pd.get_dummies(X['eyecolor'])\n",
        "Y = df_train['gender']\n",
        "X_01= df_test[['height', 'eyecolor']]\n",
        "X_01['eyecolor'] = pd.get_dummies(X_01['eyecolor'])\n",
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=50,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "df_gender_test_=df_test['gender'].replace('male',1).replace('female',0)\n",
        "x_train=X.iloc[:,:]\n",
        "y_train=np.array(df_gender_train_)\n",
        "y_test=np.array(df_gender_test_)\n",
        "x_test=X_01.iloc[:,:]\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "print('testing accuracy:',accuracy_score(y_test,y_pred_test))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6271261860e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_gender_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"\"\"\n\u001b[1;32m    994\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 995\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    323\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 932\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC8Ipx9QrePH",
        "colab_type": "text"
      },
      "source": [
        "**Question 4.b**  \n",
        "What is the accuracy on the test set using the log of height values (applied to both training and testing sets) and eye color as a one-hot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFvzNv6O1DG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "7a50878c-b7c5-4dda-8107-4a08f43622ff"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_train['height']=np.log(df_train['height'])\n",
        "df_test['height']=np.log(df_test['height'])\n",
        "X = df_train[['height', 'eyecolor']]\n",
        "X['eyecolor'] = pd.get_dummies(X['eyecolor'])\n",
        "Y = df_train['gender']\n",
        "X_01= df_test[['height', 'eyecolor']]\n",
        "X_01['eyecolor'] = pd.get_dummies(X_01['eyecolor'])\n",
        "clf2=MLPClassifier(hidden_layer_sizes=(10),max_iter=50,\n",
        "                    solver='sgd',verbose=1,random_state=1)\n",
        "df_gender_train_=df_train['gender'].replace('male',1).replace('female',0)\n",
        "df_gender_test_=df_test['gender'].replace('male',1).replace('female',0)\n",
        "x_train=X.iloc[:,:]\n",
        "y_train=np.array(df_gender_train_)\n",
        "y_test=np.array(df_gender_test_)\n",
        "x_test=X_01.iloc[:,:]\n",
        "clf2.fit(x_train,y_train)\n",
        "y_pred_train=clf2.predict(x_train)\n",
        "y_pred_test=clf2.predict(x_test)\n",
        "print('testing accuracy:',accuracy_score(y_test,y_pred_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fb01800077a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_gender_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"\"\"\n\u001b[1;32m    994\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 995\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    323\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 932\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYm2jqnprePI",
        "colab_type": "text"
      },
      "source": [
        "**Question 4.c**  \n",
        "What is the accuracy on the test set using the Z-score of height values and eye color as a one-hot? \n",
        "\n",
        "Z-score is a normalization function. It is the value of a feature minus the average value for that feature (in the training set), divided by the standard deviation of that feature (in the training set). Remember that, whenever applying a function to a feature in the training set, it also has to be applied to that same feature in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P3mDjF6N1DoN",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9qwu_9rePJ",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 5 ###\n",
        "Repeat question 4 for exercise hours + eye color"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAgHz_r-1EMR",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYVuaPWgrePL",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 6###\n",
        "Combine the features from question 3, 4, and 5 (year, eyecolor, exercise, height, exercise hours). For numeric features use the best normalization method from questions 4 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iAiFhlFrePM",
        "colab_type": "text"
      },
      "source": [
        "**Question 6.a**  \n",
        "What was the NN accuracy on the test set using the single 10 node hidden layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QuLJ6sTB1FfN",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jusc-kofrePP",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Question 7- Bonus (10%)###\n",
        "Can you improve your test set prediction accuracy by 5% or more?  \n",
        "\n",
        "See how close to that milestone of improvement you can get by modifying the tuning parameters of  Neural Networks (the number of hidden layers, number of hidden nodes in each layer, the learning rate aka mu). A great guide to tuning parameters is explained in this guide: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf. \n",
        "\n",
        "While the guide is specific to SVM and in particular the C and gamma parameters of the RBF kernel, the method applies to generally to any ML technique with tuning parameters.\n",
        "\n",
        "Please also write a paragraph in a markdown cell below with an explanation of your approach and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qvpoUdeq1GsX",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}